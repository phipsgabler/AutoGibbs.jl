using DataStructures: DefaultDict
using Distributions
using DynamicPPL
using Turing.RandomMeasures


export conditionals, sampled_values


"""
    abstract type Cont end

`Cont` is an analytic representation of joint distribution represented in a `Graph` -- a function
from a dictionary of variable assignments to a log probability.  (For lack of a better term, I call
these functions "continuations".)

Each `Call` gets associated with a `Transformation`, and each tilde statement with a
`LogLikelihood`.  Those are both callable with a dict argument.  SSA variables get transformed to
either a `Fixed` value for constants, a `Variable` for assignments of random variables, or the
`Cont` they come from.

Something like this:

```
‚ü®28‚ü© => logpdf(Normal(), Œ∏[Œº[2]])
‚ü®29‚ü© => getindex(‚ü®8‚ü©(array initializer with undefined values, 2), getfield(iterate(Colon()(1, 2), getfield(iterate(Colon()(1, 2)), 2)), 1))
‚ü®31‚ü© => logpdf(Dirichlet(2, 1.0), Œ∏[w])
‚ü®32‚ü© => Array{Int64,1}(array initializer with undefined values, length([0.1, -0.05, 1.0]))
‚ü®33‚ü© => Colon()(1, length([0.1, -0.05, 1.0]))
‚ü®34‚ü© => iterate(Colon()(1, length([0.1, -0.05, 1.0])))
‚ü®36‚ü© => getfield(iterate(Colon()(1, length([0.1, -0.05, 1.0]))), 1)
‚ü®37‚ü© => getfield(iterate(Colon()(1, length([0.1, -0.05, 1.0]))), 2)
‚ü®42‚ü© => logpdf(DiscreteNonParametric(Œ∏[w]), Œ∏[z[1]])
```

for 

```
‚ü®28‚ü© = Œº[2] ~ Normal() ‚Üí -1.2107564627453093
‚ü®29‚ü© = Œº[2] = getindex(‚ü®9‚ü©, ‚ü®23‚ü©) ‚Üí -1.2107564627453093
‚ü®31‚ü© = w ~ Dirichlet(‚ü®4‚ü©, 1.0) ‚Üí [0.7023731332410442, 0.2976268667589558]
‚ü®32‚ü© = Array{Int64,1}(array initializer with undefined values, ‚ü®7‚ü©) ‚Üí [139815315085536, 139815315085552, 139815315085568]
‚ü®33‚ü© = Colon()(1, ‚ü®7‚ü©) ‚Üí 1:3
‚ü®34‚ü© = iterate(‚ü®33‚ü©) ‚Üí (1, 1)
‚ü®36‚ü© = getfield(‚ü®34‚ü©, 1) ‚Üí 1
‚ü®37‚ü© = getfield(‚ü®34‚ü©, 2) ‚Üí 1
‚ü®42‚ü© = z[1] ~ DiscreteNonParametric(‚ü®31‚ü©) ‚Üí 2
```

where `Œ∏` stands for the environment of random variable assignments.
"""
abstract type Cont end


abstract type ArgSpec <: Cont end

struct Fixed{T} <: ArgSpec
    value::T
end
Base.show(io::IO, arg::Fixed) = print(io, arg.value)
(arg::Fixed)(Œ∏) = arg.value

struct Variable{TV<:VarName} <: ArgSpec
    vn::TV
end
Base.show(io::IO, arg::Variable) = print(io, "Œ∏[", arg.vn, "]")
(arg::Variable)(Œ∏) = _lookup(Œ∏, arg.vn)

function _lookup(Œ∏, varname)
    if haskey(Œ∏, varname)
        result = getindex(Œ∏, varname)
        return result
    else
        # in the case of looking up x[i] with stored x,
        # simply do it the slow way and check all elements
        for (vn, value) in Œ∏
            if DynamicPPL.subsumes(vn, varname)
                result = foldl((x, i) -> getindex(x, i...),
                               DynamicPPL.getindexing(varname),
                               init=value)
                return result
            end
        end
        throw(BoundsError(Œ∏, varname))
    end
end


struct Transformation{TF, N, TArgs<:NTuple{N, Cont}} <: Cont
    f::TF
    args::TArgs
end

function Base.show(io::IO, t::Transformation)
    print(io, t.f, "(")
    join(io, t.args, ", ")
    print(io, ")")
end

(t::Transformation)(Œ∏) = t.f((arg(Œ∏) for arg in t.args)...)

function (t::Transformation{typeof(getindex)})(Œ∏)
    # intercept this to simplify getindex(x, i) to direct lookup of x[i]
    array, indexing = first(t.args), Base.tail(t.args)
    if array isa Variable
        actual_vn = VarName(array.vn, (Tuple(ix(Œ∏) for ix in indexing),))
        return _lookup(Œ∏, actual_vn)
    else
        return t.f((arg(Œ∏) for arg in t.args)...)
    end
end


struct LogLikelihood{TDist, TF, TArgs, TVal} <: Cont
    dist::TDist
    f::TF # function that was used to construct the distribution
    args::TArgs
    value::TVal
    
    function LogLikelihood(dist::Distribution, f, args::NTuple{N, Cont}, value) where {N}
        return new{typeof(dist), typeof(f), typeof(args), typeof(value)}(
            dist, f, args, value)
    end
end

function Base.show(io::IO, ‚Ñì::LogLikelihood{D}) where {D}
    print(io, "logpdf(", ‚Ñì.f, "(")
    join(io, ‚Ñì.args, ", ")
    print(io, "), ", ‚Ñì.value, ")")
end

(‚Ñì::LogLikelihood{D})(Œ∏) where {D} = begin
    logpdf(‚Ñì.f((arg(Œ∏) for arg in ‚Ñì.args)...), ‚Ñì.value(Œ∏))
end


_init(::Tuple{}) = ()
_init(t::Tuple{Any}) = ()
_init(t::Tuple) = (first(t), _init(Base.tail(t))...)
_last(::Tuple{}) = ()
_last((x,)::Tuple{Any}) = x
_last(t::Tuple) = _last(Base.tail(t))



"""
    continuations(graph)

Assign to each node in the graph a `Cont` object, representing a function from a variable assignment
(Œ∏) to the value of an expression or the log-likelihood in case of a tilde statement.
"""
function continuations(graph)
    c = SortedDict{Reference, Cont}()
    
    function convertarg(arg)
        if arg isa Reference
            cont = c[arg]
            stmt = graph[arg]
            if cont isa LogLikelihood && !isnothing(stmt.vn)
                return Variable(stmt.vn)
            elseif cont isa Transformation{typeof(getindex)} && !isnothing(stmt.definition)
                # two cases:
                # - vn[ix] = getindex(<a>, ix) where <a> = vn ~ D: then we keep getindex(Œ∏[a], ix)
                #   (the whole array was sampled by the tilde)
                # - vn[ix] = getindex(<a>, ix) where <scalar> = vn[ix] ~ D: then we rewrite to
                # `getindex(Œ∏[a], ix)`  (the array element was sampled individually)
                # this truncation is realized by `_init`, which discards the last element of a tuple,
                # and maps the empty tuple to itself.
                location, ref = stmt.definition
                array, ix = stmt.args[1], stmt.args[2:end]
                parent_vn = VarName(graph[ref].vn, _init(DynamicPPL.getindexing(graph[ref].vn)))
                parent_array = Variable(parent_vn)
                return Transformation(getindex, (parent_array, convertarg.(ix)...))
            else
                return cont
            end
        else
            return Fixed(arg)
        end
    end
    
    for (ref, stmt) in graph
        if stmt isa Union{Assumption, Observation}
            dist_stmt = stmt.dist
            dist = getvalue(dist_stmt)
            if dist_stmt isa Call
                f, args = dist_stmt.f, convertarg.(dist_stmt.args)
            elseif f isa Constant
                # fake a constant here... `getindex(Ref(x)) == x`
                f = getindex
                args = (Fixed(Ref(dist)),)
            end
            value = Variable(stmt.vn)
            c[ref] = LogLikelihood(dist, f, args, value)
        elseif stmt isa Call
            f, args = stmt.f, convertarg.(stmt.args)
            c[ref] = Transformation(f, args)
        elseif stmt isa Constant
            c[ref] = Fixed(getvalue(stmt))
        end
    end

    return c
end

"""
    conditionals(graph, varname)

Calculate the `GibbsConditional`s of all variables in `graph` that match `varname`.  There can 
be multiple observations for which this holds, so a dictionary is returned.
"""
function conditionals(graph, varname)
    dists = Dict{VarName, LogLikelihood}()
    blankets = DefaultDict{VarName, Vector{Pair{VarName, LogLikelihood}}}(
        Vector{Pair{Tuple, LogLikelihood}})
    conts = continuations(graph)
    
    for (ref, stmt) in graph
        if stmt isa Union{Assumption, Observation} && !isnothing(stmt.vn)
            # record distribution of this tilde if it matches the searched vn
            if DynamicPPL.subsumes(varname, stmt.vn)
                dists[stmt.vn] = conts[ref]
            end
            
            # add likelihood to all parents of which this RV is in the blanket
            for (pvn, p) in parent_variables(graph, stmt)
                for vn in keys(dists)
                    if DynamicPPL.subsumes(vn, p.vn)
                        push!(blankets[vn], pvn => conts[ref])
                        break
                    end
                end
            end
        end
    end

    return Dict(vn => GibbsConditional(vn, d, blankets[vn]) for (vn, d) in dists)
end


"""
    sampled_values(graph)

Extract the values of all observed and assumed random variables in the graph, including their
occuring parts.
"""
function sampled_values(graph)
    Œ∏ = Dict{VarName, Any}()
    for (ref, stmt) in graph
        if stmt isa Union{Assumption, Observation} && !isnothing(stmt.vn)
            Œ∏[stmt.vn] = tovalue(graph, getvalue(stmt))
        elseif stmt isa Call && !isnothing(stmt.definition)
            # remember all intermediate RV values (including redundant `getindex` calls,
            # for simplicity)
            vn, _ = stmt.definition
            Œ∏[vn] = getvalue(stmt)
        end
    end

    return Œ∏
end


struct GibbsConditional{
    TVar<:VarName,
    TBase<:LogLikelihood,
    TBlanket}

    vn::TVar
    base::TBase
    blanket::TBlanket
end

function Base.show(io::IO, c::GibbsConditional)
    print(io, c.base)
    if !isempty(c.blanket)
        print(io, " + ")
        join(io, (Œ≤ for (vn, Œ≤) in c.blanket), " + ")
    end
end


"""
    (c::GibbsConditional)(Œ∏)

Return the conditional distribution of `vn` given the values fixed in `Œ∏`, calculated by 
normalization over the conditional distribution of `vn` itlself and its Markov blanket.

Constructed as

    P[X = x | Œ∏] ‚àù P[X = x | parents(X)] * P[children(X) | parents(children(X))]

equivalent to a distribution `D` such that

    logpdf(D, x) = ‚Ñì(x | Œ∏) + ‚àë blanket·µ¢(x | Œ∏)

where the factors `blanket·µ¢` are the log probabilities in the Markov blanket.

This only works on discrete distributions, either scalar ones (resulting in a 
`DiscreteNonparametric`) or products of them (resulting in a `Product` of `DiscreteNonparametric`).
"""
function (c::GibbsConditional{V, L})(Œ∏) where {
    V<:VarName, L<:LogLikelihood{<:DiscreteUnivariateDistribution}}

    Œ© = try
        support(c.base.dist)
    catch
        throw(ArgumentError("Unable to get the support of $(c.base.dist) (probably infinite)!"))
    end

    Œ∏s_on_support = fixvalues(Œ∏, c.vn => Œ©)
    logtable = [c.base(Œ∏‚Ä≤) + reduce(+, (Œ≤(Œ∏‚Ä≤) for (vn, Œ≤) in c.blanket), init=0.0)
                for Œ∏‚Ä≤ in Œ∏s_on_support]
    conditional = DiscreteNonParametric(Œ©, softmax!(logtable))
    return conditional
end

# `Product`s can be treated as an array of iid variables
function (c::GibbsConditional{V, L})(Œ∏) where {
    V<:VarName, L<:LogLikelihood{<:Product}}

    independent_distributions = c.base.dist.v
    Œ©s = try
        support.(independent_distributions)
    catch
        throw(ArgumentError("Unable to get the support of $(c.base.dist) (probably infinite)!"))
    end
    
    conditionals = similar(Œ©s, DiscreteNonParametric)
    
    for index in eachindex(Œ©s, independent_distributions, conditionals)
        sub_vn = DynamicPPL.VarName(c.vn, (DynamicPPL.getindexing(c.vn)..., (index,)))
        Œ∏s_on_support = fixvalues(Œ∏, sub_vn => Œ©s[index])
        logtable = map(Œ∏s_on_support) do Œ∏‚Ä≤
            c.base(Œ∏‚Ä≤) + reduce(+, (Œ≤(Œ∏‚Ä≤) for (vn, Œ≤) in c.blanket if vn == sub_vn), init=0.0)
        end
        conditionals[index] = DiscreteNonParametric(Œ©s[index], softmax!(vec(logtable)))
    end

    return Product(conditionals)
end


# # Special treatment for CRP variables: calculate likelihoods as normal for truncated support
# # (covering all existing clusters), and marginalize the creation of a new cluster
# function (c::GibbsConditional{V, L})(Œ∏) where {
#     V<:VarName, L<:LogLikelihood{<:ChineseRestaurantProcess}}
#     Œ© = support(c.base.dist)
#     Œ©_init, Œ©_last = Œ©[1:end-1], Œ©[end]

#     Œ∏s_on_init = fixvalues(Œ∏, c.vn => Œ©_init)
#     logtable_init = Float64[c.base(Œ∏‚Ä≤) + reduce(+, (Œ≤(Œ∏‚Ä≤) for (vn, Œ≤) in c.blanket), init=0.0)
#                             for Œ∏‚Ä≤ in Œ∏s_on_support]
    
#     Œ∏_on_last = fixvalues(Œ∏, c.vn => [Œ©_last])
#     log_last = _estimate_last_likelihood(c, Œ∏_on_last)
#     conditional = DiscreteNonParametric(Œ©, softmax!(push!(logtable_init, log_last)))
#     return conditional
# end


# """
# Estimate the "new cluster" likelihood of a CRP, given through

#     ùìÖ(z‚Çô = K + 1 | z‚ÇÅ, ..., z‚Çô‚Çã‚ÇÅ, Œº, x‚Çô) ‚àù (‚àè_{i = z ‚â• n} ùìÖ(z·µ¢ | z‚ÇÅ,...,z·µ¢)) ùìÖ(x‚Çô | z‚Çô = K + 1, Œº),

# by approximating

#     ùìÖ(x‚Çô | z‚Çô = K + 1, Œº) = ‚à´ ùìÖ(x‚Çô | Œº = m) dm ‚âà ùìÖ(x‚Çô | m)

# where Law(m) = Law(Œº).
# """
# function _estimate_last_likelihood(c, Œ∏)
#     l = c.base(Œ∏)
    
#     for (ix, Œ≤) in c.blanket
#         if Œ≤ isa LogLikelihood{<:ChineseRestaurantProcess} && Œ≤.dist.rpm == c.base.rpm
#             l += Œ≤(Œ∏)
#         else
            
#             conditioned_dist = Œ≤.f((arg(Œ∏) for arg in Œ≤.args)...)
#             sample = rand(conditioned_dist)
#             l += logpdf(conditioned_dist, sample)
#         end
#     end
    
#     return l
# end


(c::GibbsConditional)(Œ∏) =
    throw(ArgumentError("Cannot condition a non-discrete or non-univariate distribution $(c.base)."))


"""Produce `|Œ©|` copies of `Œ∏` with the `fixedvn` entries fixed to the values in the support `Œ©`."""
function fixvalues(Œ∏, (source_vn, Œ©))
    # "source" is the value stored in Œ©; "target" is the matching value in Œ∏.

    result = [copy(Œ∏) for _ in Œ©]
    
    for (Œ∏‚Ä≤, value) in zip(result, Œ©)
        for target_vn in keys(Œ∏‚Ä≤)
            source_subsumes_target = DynamicPPL.subsumes(source_vn, target_vn)
            target_subsumes_source = DynamicPPL.subsumes(target_vn, source_vn)
            
            if source_subsumes_target && target_subsumes_source
                # target <- source, target[i] <- source[i]
                # both indices match -- we just update the complete thing
                Œ∏‚Ä≤[target_vn] = value
                
            elseif target_subsumes_source
                # target <- source[i], target[i] <- source[i][j]
                # updating the target from a part of the source with "copy on write",
                # target[i][j] = source[i][j] <=> setindex!(copy(target[i]), source[i][j], j)

                # NB: we index into the target by the source index!
                target_indexing = DynamicPPL.getindexing(source_vn)
                ti_init, ti_last = _splitindexing(target_indexing)
                target = Œ∏‚Ä≤[target_vn]
                initial_target = foldl((x, i) -> getindex(x, i...),
                                       ti_init,
                                       init=target)
                if isnothing(ti_last)
                    Œ∏‚Ä≤[target_vn] = initial_target
                else
                    target = setindex!(copy(initial_target), value, ti_last...)
                    Œ∏‚Ä≤[target_vn] = target
                end
            elseif source_subsumes_target
                # target[i][j] <- source[i]
                # setting the target to part of the source array
                # target[i][j] = source[i][j]
                
                target_indexing = DynamicPPL.getindexing(target_vn)
                target = foldl((x, i) -> getindex(x, i...),
                               target_indexing,
                               init=value)
                Œ∏‚Ä≤[target_vn] = target
            end
        end
    end

    return result
end


_splitindexing(::NTuple{0}) = (), nothing
_splitindexing((i,)::NTuple{1}) = (), i
_splitindexing((i, j)::NTuple{2}) = (i,), j
_splitindexing(t::Tuple) = let (init, last) = _splitindexing(Base.tail(t))
    (first(t), init...), last
end


# from https://github.com/JuliaStats/StatsFuns.jl/blob/master/src/basicfuns.jl#L259
function softmax!(x::AbstractArray{<:AbstractFloat})
    u = maximum(x)
    s = zero(u)
    
    @inbounds for i in eachindex(x)
        s += (x[i] = exp(x[i] - u))
    end
    
    s‚Åª¬π = inv(s)
    
    @inbounds for i in eachindex(x)
        x[i] *= s‚Åª¬π
    end
    
    return x
end

softmax(x::AbstractArray{<:AbstractFloat}) = softmax!(copy(x))



# D_w = Dirichlet(2, 1.0)
# w = rand(D_w)
# D_p = DiscreteNonParametric([0.3, 0.6], w)
# p = rand(D_p)
# D_x = Bernoulli(p)
# x = rand(D_x)
# conditioned(D_p, [logpdf(D_x, x)])
